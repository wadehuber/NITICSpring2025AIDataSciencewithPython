{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP)\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "YnQ2f0zkJ2hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "TQ8UhVozJ28F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLCfWhcrJ2BU",
        "outputId": "5f744b39-7be2-461d-e6ff-5a5c5ab1049a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'universe', 'is', 'change', ';', 'our', 'life', 'is', 'what', 'our', 'thoughts', 'make', 'it', '.']\n",
            "  sentence 0: They gradually ascended for half a mile, and then found themselves at the top of a considerable eminence, where the wood ceased, and the eye was instantly caught by Pemberley House, situated on the opposite side of a valley, into which the road with some abruptness wound.\n",
            "  sentence 1: It was a large, handsome, stone building, standing well on rising ground, and backed by a ridge of high woody hills;—and in front, a stream of some natural importance was swelled into greater, but without any artificial appearance.\n",
            "  sentence 2: Its banks were neither formal, nor falsely adorned.\n",
            "  sentence 3: Elizabeth was delighted.\n",
            "  sentence 4: She had never seen a place where nature had done more, or where natural beauty had been so little counteracted by an awkward taste.\n",
            "  sentence 5: They were all of them warm in her admiration; and at that moment she felt that to be mistress of Pemberley might be something!\n"
          ]
        }
      ],
      "source": [
        "# Word tokenization with NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The universe is change; our life is what our thoughts make it.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "quote = 'They gradually ascended for half a mile, and then found ' + \\\n",
        "        'themselves at the top of a considerable eminence, where ' + \\\n",
        "        'the wood ceased, and the eye was instantly caught by ' + \\\n",
        "        'Pemberley House, situated on the opposite side of a ' + \\\n",
        "        'valley, into which the road with some abruptness wound. ' + \\\n",
        "        'It was a large, handsome, stone building, standing well ' + \\\n",
        "        'on rising ground, and backed by a ridge of high woody ' + \\\n",
        "        'hills;—and in front, a stream of some natural importance ' + \\\n",
        "        'was swelled into greater, but without any artificial ' + \\\n",
        "        'appearance. Its banks were neither formal, nor falsely ' + \\\n",
        "        'adorned. Elizabeth was delighted. She had never seen a ' + \\\n",
        "        'place where nature had done more, or where natural ' + \\\n",
        "        'beauty had been so little counteracted by an awkward ' + \\\n",
        "        'taste. They were all of them warm in her admiration; ' + \\\n",
        "        'and at that moment she felt that to be mistress of ' + \\\n",
        "        'Pemberley might be something!'\n",
        "        # Austen, Jane. Pride and Prejudice. 1813, vol. 3, ch. 1\n",
        "\n",
        "sentences = sent_tokenize(quote)\n",
        "for index, sentence in enumerate(sentences):\n",
        "    print(f'  sentence {index}: {sentence}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "bYLxd7j_LEFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "evals = ['This course is absolutely amazing!',\n",
        "         'Absolute waste of time.  I learned nothing.',\n",
        "         'It was fine, but moved too slow',\n",
        "         'I like turtles!']\n",
        "\n",
        "for eval in evals:\n",
        "    scores = sia.polarity_scores(eval)\n",
        "    print(f'{eval}\\n\\t{scores}')"
      ],
      "metadata": {
        "id": "vIP2rkwbLEOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92cae0a-7919-4853-9c7b-e6ab44e7d67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This course is absolutely amazing!\n",
            "\t{'neg': 0.0, 'neu': 0.477, 'pos': 0.523, 'compound': 0.6581}\n",
            "Absolute waste of time.  I learned nothing.\n",
            "\t{'neg': 0.359, 'neu': 0.641, 'pos': 0.0, 'compound': -0.4215}\n",
            "It was fine, but moved too slow\n",
            "\t{'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.1027}\n",
            "I like turtles!\n",
            "\t{'neg': 0.0, 'neu': 0.264, 'pos': 0.736, 'compound': 0.4199}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing for Statistical NLP\n",
        "Clean text for statistical models like Naive Bayes or word frequency analysis\n"
      ],
      "metadata": {
        "id": "WxvA2wHM6fh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "try:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)  # For word_tokenize\n",
        "except:\n",
        "    print(\"NLTK download failed—ensure internet connection!\")\n",
        "\n",
        "# Initialize preprocessing tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sample text for preprocessing\n",
        "text = \"Follies and nonsense, whims and inconsistencies do divert me, I own, and I laugh at them whenever I can.\"\n",
        "\n",
        "# Step-by-step preprocessing\n",
        "tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]  # Remove punctuation, stop words\n",
        "stemmed = [stemmer.stem(w) for w in filtered]  # Stem words\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in filtered]  # Lemmatize words\n",
        "\n",
        "# Word frequency for statistical NLP\n",
        "from collections import Counter\n",
        "freq = Counter(lemmatized)\n",
        "\n",
        "# Display results\n",
        "print(\"Step 1 - Tokenized & Lowercased:\", tokens)\n",
        "print(\"Step 2 - Filtered (No Punctuation/Stop Words):\", filtered)\n",
        "print(\"Step 3 - Stemmed (Porter):\", stemmed)\n",
        "print(\"Step 4 - Lemmatized (WordNet):\", lemmatized)\n",
        "print(\"Step 5 - Word Frequency (Statistical NLP):\", freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qNQ16PJ6fzA",
        "outputId": "83f03cdb-cbdf-4180-a763-7f858e661ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 - Tokenized & Lowercased: ['follies', 'and', 'nonsense', ',', 'whims', 'and', 'inconsistencies', 'do', 'divert', 'me', ',', 'i', 'own', ',', 'and', 'i', 'laugh', 'at', 'them', 'whenever', 'i', 'can', '.']\n",
            "Step 2 - Filtered (No Punctuation/Stop Words): ['follies', 'nonsense', 'whims', 'inconsistencies', 'divert', 'laugh', 'whenever']\n",
            "Step 3 - Stemmed (Porter): ['folli', 'nonsens', 'whim', 'inconsist', 'divert', 'laugh', 'whenev']\n",
            "Step 4 - Lemmatized (WordNet): ['folly', 'nonsense', 'whim', 'inconsistency', 'divert', 'laugh', 'whenever']\n",
            "Step 5 - Word Frequency (Statistical NLP): Counter({'folly': 1, 'nonsense': 1, 'whim': 1, 'inconsistency': 1, 'divert': 1, 'laugh': 1, 'whenever': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram Analysis\n",
        "This section preprocesses a sample text by tokenizing, filtering, stemming, and lemmatizing, then generates bigrams and trigrams, and computes word frequencies using NLTK.\n"
      ],
      "metadata": {
        "id": "mbTM1nFU8iI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "text = \"Natural language processing is fascinating.  Absolutely fascinating!\"\n",
        "tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]  # Remove punctuation, stop words\n",
        "stemmed = [stemmer.stem(w) for w in filtered]  # Stem words\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in filtered]  # Lemmatize words\n",
        "\n",
        "# N-Gram generation\n",
        "bigrams = list(ngrams(lemmatized, 2))  # Bigrams from lemmatized tokens\n",
        "trigrams = list(ngrams(lemmatized, 3))  # Trigrams from lemmatized tokens\n",
        "\n",
        "# Word frequency for statistical NLP\n",
        "freq = Counter(tokens)\n",
        "\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)\n",
        "print(\"Word Frequency:\", freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3h3ubp38iRr",
        "outputId": "502a44cd-7295-4a62-85b8-76f97b426566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'fascinating'), ('fascinating', 'absolutely'), ('absolutely', 'fascinating')]\n",
            "Trigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'fascinating'), ('processing', 'fascinating', 'absolutely'), ('fascinating', 'absolutely', 'fascinating')]\n",
            "Word Frequency: Counter({'fascinating': 2, 'natural': 1, 'language': 1, 'processing': 1, 'is': 1, '.': 1, 'absolutely': 1, '!': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Vectorization with CountVectorizer\n",
        "This example demonstrates the use of scikit-learn's CountVectorizer to convert a small corpus of text into a numerical feature matrix. It tokenizes three sentences, builds a vocabulary of unique words, and transforms the text into a bag-of-words representation, printing both the feature names (words) and the resulting array."
      ],
      "metadata": {
        "id": "26f8WucCA8Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"I am malicious because I am miserable.\",\n",
        "    \"The fallen angel becomes a malignant devil.\",\n",
        "    \"Life, although it may only be an accumulation of anguish, is dear to me.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50HCQccQA8js",
        "outputId": "4e5d0bfd-d70a-4e9b-dbd3-151663765fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['accumulation' 'although' 'am' 'an' 'angel' 'anguish' 'be' 'because'\n",
            " 'becomes' 'dear' 'devil' 'fallen' 'is' 'it' 'life' 'malicious'\n",
            " 'malignant' 'may' 'me' 'miserable' 'of' 'only' 'the' 'to']\n",
            "[[0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0]\n",
            " [1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Vectorization Example\n",
        "This example showcases TF-IDF vectorization using scikit-learn’s TfidfVectorizer. It transforms a small text corpus into a weighted matrix, emphasizing rare terms over common ones, and prints the feature names (vocabulary) and the resulting TF-IDF array."
      ],
      "metadata": {
        "id": "_W-w31ozDBFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define a sample corpus\n",
        "corpus = [\n",
        "    \"I am malicious because I am miserable\",\n",
        "    \"The fallen angel becomes a malignant devil\",\n",
        "    \"Life is dear to me despite its anguish\"\n",
        "]\n",
        "\n",
        "# Initialize and apply TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Display results\n",
        "print(\"Feature Names (Words):\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMsWLj3eDBM9",
        "outputId": "b6d6337b-8814-475a-d4f1-4da3b4c263f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Words): ['am' 'angel' 'anguish' 'because' 'becomes' 'dear' 'despite' 'devil'\n",
            " 'fallen' 'is' 'its' 'life' 'malicious' 'malignant' 'me' 'miserable' 'the'\n",
            " 'to']\n",
            "TF-IDF Matrix:\n",
            " [[0.75592895 0.         0.         0.37796447 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.37796447 0.         0.         0.37796447 0.         0.        ]\n",
            " [0.         0.40824829 0.         0.         0.40824829 0.\n",
            "  0.         0.40824829 0.40824829 0.         0.         0.\n",
            "  0.         0.40824829 0.         0.         0.40824829 0.        ]\n",
            " [0.         0.         0.35355339 0.         0.         0.35355339\n",
            "  0.35355339 0.         0.         0.35355339 0.35355339 0.35355339\n",
            "  0.         0.         0.35355339 0.         0.         0.35355339]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Sentiment Classification\n",
        "This example demonstrates text classification using scikit-learn’s Multinomial Naive Bayes in a pipeline with CountVectorizer. It trains on a small sentiment dataset (positive/negative reviews) and predicts labels for test samples, showcasing probabilistic classification in action.\n",
        "\n"
      ],
      "metadata": {
        "id": "_F71GE3fDdzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# More robust training data with distinct sentiment cues\n",
        "X_train = [\n",
        "    \"I love this amazing product\",\n",
        "    \"Terrible experience with this junk\",\n",
        "    \"Fantastic service and quality\",\n",
        "    \"Horrible and utterly disappointing\",\n",
        "    \"Really joyful happy purchase\",\n",
        "    \"Awful disgusting bad service\"\n",
        "]\n",
        "y_train = [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\"]\n",
        "X_test = [\"This is wonderful\", \"Totally dreadful\"]\n",
        "\n",
        "# Create and train the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),  # Convert text to word counts\n",
        "    ('classifier', MultinomialNB())     # Apply Naive Bayes classifier\n",
        "])\n",
        "\n",
        "# Training and prediction\n",
        "pipeline.fit(X_train, y_train)\n",
        "predictions = pipeline.predict(X_test)\n",
        "\n",
        "# Display results\n",
        "print(\"Test Samples:\", X_test)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "vectorizer = pipeline.named_steps['vectorizer']\n",
        "classifier = pipeline.named_steps['classifier']\n",
        "print(\"\\nVocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Class Probabilities for Test Samples:\\n\", pipeline.predict_proba(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLq7YHKxDd5t",
        "outputId": "5688ea81-986c-4642-ef9c-ea47904750a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Samples: ['This is wonderful', 'Totally dreadful']\n",
            "Predictions: ['positive' 'negative']\n",
            "\n",
            "Vocabulary: ['amazing' 'and' 'awful' 'bad' 'disappointing' 'disgusting' 'experience'\n",
            " 'fantastic' 'happy' 'horrible' 'joyful' 'junk' 'love' 'product'\n",
            " 'purchase' 'quality' 'really' 'service' 'terrible' 'this' 'utterly'\n",
            " 'with']\n",
            "Class Probabilities for Test Samples:\n",
            " [[0.49275362 0.50724638]\n",
            " [0.5        0.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parts of Speech Tagging with NLTK\n",
        "\n",
        "Description: This cell demonstrates basic POS tagging using NLTK’s pre-trained averaged_perceptron_tagger. It tokenizes a sample sentence (\"The quick brown fox jumps over the lazy dog\") and assigns grammatical tags (e.g., noun, verb, adjective) to each word, printing the resulting tagged list."
      ],
      "metadata": {
        "id": "cNV4PlS0K1-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt_tab')                  # For tokenization\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # For POS tagging\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc0YjHdPK2Fx",
        "outputId": "c1907983-f1c6-4164-8ce7-5e17865b00a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition with spaCy\n",
        "This example demonstrates how to identify and label entities like organizations, locations, dates, and monetary values in a sentence using spaCy's pre-trained language model."
      ],
      "metadata": {
        "id": "bY2wCd5WQW3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sW-6wH-WS0PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = 'President Donald Trump unveiled 54% tariffs on all Chinese imports into the United States ' + \\\n",
        "       'Wednesday as part of his sweeping “Liberation Day” reset of American trade global policy.'\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"→\", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_QEDYcxQXAY",
        "outputId": "5762efa0-5663-4393-b721-851e5d5eba2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Donald Trump → PERSON\n",
            "54% → PERCENT\n",
            "Chinese → NORP\n",
            "the United States → GPE\n",
            "Wednesday → DATE\n",
            "American → NORP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Context-Free Grammars (CFGs) with NLTK\n",
        "CFGs define the syntactic structure of sentences using formal production rules. This example sets up a grammar and demonstrates how to generate parse trees, which help visualize sentence structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "TblVImshS2wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import CFG\n",
        "from nltk.parse import ChartParser\n",
        "\n",
        "# Define a simple context-free grammar (CFG)\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det N | Det N PP\n",
        "    VP -> V NP | V NP PP\n",
        "    PP -> P NP\n",
        "    Det -> 'the' | 'a'\n",
        "    N -> 'dog' | 'cat'\n",
        "    V -> 'chased' | 'saw'\n",
        "    P -> 'in' | 'with'\n",
        "\"\"\")\n",
        "\n",
        "# Create a parser using the defined grammar\n",
        "parser = ChartParser(grammar)\n",
        "\n",
        "# Positive example (valid sentence)\n",
        "sentence1 = ['the', 'dog', 'chased', 'a', 'cat']\n",
        "print(\"Parsing valid sentence:\", ' '.join(sentence1))\n",
        "for tree in parser.parse(sentence1):\n",
        "    tree.pretty_print()\n",
        "\n",
        "# Negative example (invalid sentence - not covered by grammar)\n",
        "sentence2 = ['the', 'dog', 'barked', 'at', 'a', 'cat']\n",
        "print(\"\\nAttempting to parse invalid sentence:\", ' '.join(sentence2))\n",
        "try:\n",
        "    parsed = list(parser.parse(sentence2))\n",
        "    if parsed:\n",
        "        for tree in parsed:\n",
        "            tree.pretty_print()\n",
        "    else:\n",
        "        print(\"No parse tree — sentence doesn't match the grammar rules.\")\n",
        "except Exception as e:\n",
        "    print(\"Error during parsing:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqzidU6VS24y",
        "outputId": "e8039582-bc79-446b-f6bb-20fa946adcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing valid sentence: the dog chased a cat\n",
            "              S               \n",
            "      ________|_____           \n",
            "     |              VP        \n",
            "     |         _____|___       \n",
            "     NP       |         NP    \n",
            "  ___|___     |      ___|___   \n",
            "Det      N    V    Det      N \n",
            " |       |    |     |       |  \n",
            "the     dog chased  a      cat\n",
            "\n",
            "\n",
            "Attempting to parse invalid sentence: the dog barked at a cat\n",
            "Error during parsing: Grammar does not cover some of the input words: \"'barked', 'at'\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependency Parsing with spaCy\n",
        "This example analyzes the grammatical structure of a sentence by identifying how each word depends on others. It uses spaCy to extract and display syntactic relationships, such as subjects, objects, and modifiers.\n",
        "\n"
      ],
      "metadata": {
        "id": "8F6FpNMAUMhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX2ZLVAFUMpc",
        "outputId": "040e2ca3-822f-49ff-fd8e-a88f2c2387f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The --> det --> fox\n",
            "quick --> amod --> fox\n",
            "brown --> amod --> fox\n",
            "fox --> nsubj --> jumps\n",
            "jumps --> ROOT --> jumps\n",
            "over --> prep --> jumps\n",
            "the --> det --> dog\n",
            "lazy --> amod --> dog\n",
            "dog --> pobj --> over\n"
          ]
        }
      ]
    }
  ]
}